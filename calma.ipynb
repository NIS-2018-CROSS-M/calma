{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "calma-wlie.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Jkk1Bh64bExg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## set up telegram notifications\n",
        "\n",
        "не очень понятно, нужно ли это. если нужно -- напишите @oserikov в телеграме, я расскажу, что сделать, чтобы присылались сообщения с качеством модели когда она отработает. "
      ]
    },
    {
      "metadata": {
        "id": "F8PdRu8ebMZB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "telegram_notifications_enabled=False\n",
        "EXP_DESCRIPTION = \"WORD LEVEL INPUT EMBEDDINGS\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c_Qm0eYmtKKB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if telegram_notifications_enabled:\n",
        "    bot_token = input(\"введите telegram bot token: \")\n",
        "    chat_id = \"292749902\" # for @oserikov"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ezipeu40Ngh5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# prepare"
      ]
    },
    {
      "metadata": {
        "id": "LQKUbkPvtYuQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DKE_XEI6MGVf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### install prereqs"
      ]
    },
    {
      "metadata": {
        "id": "4FQ2GxOmUxj5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# clone openmt-py used in calma and move it into the proper folder\n",
        "!git clone https://github.com/OpenNMT/OpenNMT-py.git\n",
        "%cd OpenNMT-py\n",
        "!git checkout -b stable-version d57fa68e6b0c2041642af40f76e1d5903c80a9b8\n",
        "%cd ..\n",
        "!mv OpenNMT-py ~\n",
        "!wget -q https://raw.githubusercontent.com/NIS-2018-CROSS-M/calma/tmp-utils/utils/onmt-decoder.py -O ~/OpenNMT-py/onmt/decoders/decoder.py\n",
        "!wget -q https://raw.githubusercontent.com/NIS-2018-CROSS-M/calma/tmp-utils/utils/onmt-opts.py -O ~/OpenNMT-py/onmt/opts.py\n",
        "\n",
        "# clone and run a tool installing pytorch 0.4.1 with cuda 9.2 into colab (maybe works on any ubuntu 16)\n",
        "!git clone https://gist.github.com/f7b7c7758a46da49f84bc68b47997d69.git colab_cuda_upgrader\n",
        "!bash colab_cuda_upgrader/pytorch041_cuda92_colab.sh\n",
        "\n",
        "\n",
        "# install dependencies used in calma project\n",
        "!{sys.executable} -m pip install configargparse\n",
        "\n",
        "# install the proper version of torchtext\n",
        "!git clone https://github.com/pytorch/text.git\n",
        "%cd text\n",
        "!{sys.executable} -m pip install .\n",
        "%cd ..\n",
        "\n",
        "# receive the calma\n",
        "!git clone https://github.com/ftyers/calma.git\n",
        "%cd calma\n",
        "!git checkout -b latest-known-version d4ce3758d06538933855f734a44630efc8e2b6b2\n",
        "%cd sharedtaskdata\n",
        "!rm onmt-data/*\n",
        "!rm results/*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oV2PmsYFMKeM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### imports"
      ]
    },
    {
      "metadata": {
        "id": "ogutrlxtyckF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import defaultdict as dd\n",
        "from random import shuffle\n",
        "import re\n",
        "import urllib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pbHwWr6F1UiD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## helping functions definitions"
      ]
    },
    {
      "metadata": {
        "id": "5ru4qyF21CyG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def initialize_data(train_src, train_tgt, valid_src, valid_tgt, prepared_training_data_prefix):\n",
        "    prepr_params = f\"-train_src {train_src} -train_tgt {train_tgt} -valid_src {valid_src} -valid_tgt {valid_tgt} -save_data {prepared_training_data_prefix}\" \n",
        "    !{sys.executable} ~/OpenNMT-py/preprocess.py {prepr_params}\n",
        "    \n",
        "    \n",
        "def train_ml(train_params):\n",
        "    train_params = \" \".join(train_params)\n",
        "    !{sys.executable} ~/OpenNMT-py/train.py {train_params}\n",
        "\n",
        "                \n",
        "def generate_predictions(generation_params, output_filename):\n",
        "    generation_params = \" \".join(generation_params)\n",
        "    !{sys.executable} ~/OpenNMT-py/translate.py {generation_params} > {output_filename}\n",
        "\n",
        "    \n",
        "def choose_best_predictions(nbest_filename, covered_filename, output_filename):\n",
        "    !cat {nbest_filename} | grep -v -P \"^\\s+\" | grep -v -P \"^\\+\" | {sys.executable} scripts/get-analyses.py 0.8 3 {covered_filename} > {output_filename}\n",
        "\n",
        "\n",
        "def score_predictions(chosen_output_filename, uncovered_filename, score_output_filename):\n",
        "    !{sys.executable} scripts/eval_tabular.py {chosen_output_filename} {uncovered_filename} \\\n",
        "     >> {score_output_filename}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RCwhLXZFE8nq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def modify_nbest(nbest_src_filename, nbest_tgt_filename, nbestModifyer):\n",
        "    with open(nbest_src_filename, 'r', encoding='utf-8') as src_f, \\\n",
        "        open(nbest_tgt_filename, 'w', encoding='utf-8') as tgt_f:\n",
        "\n",
        "        for line in src_f.readlines():\n",
        "            line = line.rstrip('\\n').rstrip('\\r')\n",
        "            if line.startswith(\"SENT \"):\n",
        "                line = nbestModifyer.sent_to_baseline_compatible(line)\n",
        "            elif re.match(\"^\\[[\\-\\+]?\\d+\\.\\d+\\]\\s\\[\", line):\n",
        "                line = nbestModifyer.hyp_to_baseline_compatible(line)\n",
        "\n",
        "            print(line, file=tgt_f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OHpWYMgkD-Uk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### def data_generation"
      ]
    },
    {
      "metadata": {
        "id": "dRu3nPDzihjx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# the method called for each non-processed training data row\n",
        "def get_data_entry(language, wordform, lemma, pos_tag, morphological_analysis):\n",
        "    lemma = ' '.join(lemma)\n",
        "    wordform = ' '.join(wordform)\n",
        "    morphological_analysis = morphological_analysis.split('|')\n",
        "    return wordform, '%s %s' % (lemma, ' '.join(['+%s' % x for x in [pos_tag] + morphological_analysis  + [\"Language=%s\" % language]]))\n",
        "\n",
        "\n",
        "def generate_onmt_data(fn, res_src_fn, res_tgt_fn, DataModifyerClass):\n",
        "    \n",
        "    modify_src_line = DataModifyerClass.modify_src_line\n",
        "    modify_tgt_line = DataModifyerClass.modify_tgt_line\n",
        "    restore_orig_src_line = DataModifyerClass.restore_src_line\n",
        "    restore_orig_tgt_line = DataModifyerClass.restore_tgt_line\n",
        "    \n",
        "    analyses = dd(set)\n",
        "\n",
        "    for line in open(fn, encoding='utf-8'):\n",
        "        line = line.rstrip('\\n').rstrip('\\r')\n",
        "        lang, wf, lemma, pos, msd = line.split('\\t')\n",
        "        wf, a = get_data_entry(lang, wf, lemma, pos, msd)\n",
        "        analyses[wf].add(a)\n",
        "    \n",
        "    tmp_src_fn = res_src_fn + \"-default\"\n",
        "    tmp_tgt_fn = res_tgt_fn + \"-default\"\n",
        "    \n",
        "    tmp_src = open(tmp_src_fn, 'w')\n",
        "    tmp_tgt = open(tmp_tgt_fn, 'w')\n",
        "    res_src = open(res_src_fn, 'w')\n",
        "    res_tgt = open(res_tgt_fn, 'w')\n",
        "    \n",
        "    analyses = list(analyses.items())\n",
        "    shuffle(analyses)\n",
        "\n",
        "    for wf, analysis in analyses:\n",
        "        for a in analysis:\n",
        "            print(wf, file = tmp_src)\n",
        "            print(a, file = tmp_tgt)\n",
        "            assert restore_orig_src_line(modify_src_line(wf)) == wf\n",
        "            assert restore_orig_tgt_line(modify_tgt_line(a)) == a\n",
        "            print(modify_src_line(wf), file = res_src)\n",
        "            print(modify_tgt_line(a), file = res_tgt)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n5XQHgTDNPUZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### def ml()"
      ]
    },
    {
      "metadata": {
        "id": "CB6i4c8TNM9a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def ml(langs, tracks, train_params, dataModifyer, nbestModifyer):\n",
        "    \n",
        "    def generate_data(orig_data_fn, res_src_fn, res_tgt_fn):\n",
        "        return generate_onmt_data(orig_data_fn, res_src_fn, res_tgt_fn, dataModifyer)\n",
        "\n",
        "    def train(train_res_src_fn, train_res_tgt_fn, val_res_src_fn, val_res_tgt_fn, save_model_fn, train_params):\n",
        "        data_fn = save_model_fn + \"-prepared_training_data\" #f\"onmt-data/{lang}-track{track}\"    \n",
        "        initialize_data(train_res_src_fn, train_res_tgt_fn, val_res_src_fn, val_res_tgt_fn, data_fn)\n",
        "\n",
        "        train_params.extend([f\"-data {data_fn}\", f\"-save_model {save_model_fn}\"])\n",
        "        train_ml(train_params)\n",
        "        !mv {save_model_fn}_step_{train_steps}.pt {save_model_fn}\n",
        "\n",
        "    \n",
        "    def predict(model_filename, input_data_filename, covered_filename, chosen_output_filename):\n",
        "        output_data_filename = f\"{input_data_filename}.out\"\n",
        "        nbest_output_filename = f\"{input_data_filename}.nbest.out\"\n",
        "        prediction_params = [\n",
        "            f\"-model {model_filename}\",\n",
        "            f\"-src {input_data_filename}\",\n",
        "            f\"-output {output_data_filename}\",\n",
        "            f\"-replace_unk\",\n",
        "            f\"-verbose\",\n",
        "            f\"-n_best 10\",\n",
        "            f\"-beam 10\"\n",
        "        ]\n",
        "\n",
        "        generate_predictions(prediction_params, nbest_output_filename)\n",
        "        nbest_output_modified_filename = nbest_output_filename+\"-modified\"\n",
        "        modify_nbest(nbest_output_filename, nbest_output_modified_filename, nbestModifyer)\n",
        "        choose_best_predictions(nbest_output_modified_filename, covered_filename, chosen_output_filename)\n",
        "    \n",
        "    for lang in langs:\n",
        "        for track in tracks:\n",
        "            train_covered_filename = f\"train/{lang}-track{track}-covered\"\n",
        "            train_uncovered_filename = f\"train/{lang}-track{track}-uncovered\"\n",
        "            val_covered_filename = f\"dev/{lang}-covered\"\n",
        "            val_uncovered_filename = f\"dev/{lang}-uncovered\"\n",
        "            test_covered_filename = f\"test/{lang}-covered\"\n",
        "            test_uncovered_filename = f\"test/{lang}-uncovered\"\n",
        "\n",
        "            train_res_src_filename = f\"onmt-data/{lang}-track{track}-src-train.txt\"\n",
        "            train_res_tgt_filename = f\"onmt-data/{lang}-track{track}-tgt-train.txt\"\n",
        "\n",
        "            val_res_src_filename = f\"onmt-data/{lang}-track{track}-src-dev.txt\"\n",
        "            val_res_tgt_filename = f\"onmt-data/{lang}-track{track}-tgt-dev.txt\"\n",
        "\n",
        "            test_res_src_filename = f\"onmt-data/{lang}-track{track}-src-test.txt\"\n",
        "            test_res_tgt_filename = f\"onmt-data/{lang}-track{track}-tgt-test.txt\"\n",
        "\n",
        "\n",
        "            generate_data(train_uncovered_filename, train_res_src_filename, train_res_tgt_filename)        \n",
        "            generate_data(val_uncovered_filename, val_res_src_filename, val_res_tgt_filename)\n",
        "\n",
        "            model_filename = f\"models/{lang}-track{track}.model\"\n",
        "            train(train_res_src_filename, train_res_tgt_filename, val_res_src_filename, val_res_tgt_filename, model_filename, train_params)\n",
        "\n",
        "\n",
        "            score_log_filename = f\"/content/calma/sharedtaskdata/{lang}-{track}-score.log\"\n",
        "            !echo \"\" > {score_log_filename}\n",
        "\n",
        "            generate_data(test_covered_filename, test_res_src_filename, test_res_tgt_filename)\n",
        "            test_pred_output_filename = f\"results/{lang}-track{track}-test-covered.sys\"\n",
        "            predict(model_filename, test_res_src_filename, test_covered_filename, test_pred_output_filename)\n",
        "            !echo \"*===QUALITY ON TEST DATA===*\" >> {score_log_filename}\n",
        "            score_predictions(test_pred_output_filename, test_uncovered_filename, score_log_filename)\n",
        "\n",
        "            val_pred_output_filename = f\"results/{lang}-track{track}-dev-covered.sys\"\n",
        "            predict(model_filename, val_res_src_filename, val_covered_filename, val_pred_output_filename)\n",
        "            !echo \"*===QUALITY ON VAL DATA===*\" >> {score_log_filename}\n",
        "            score_predictions(val_pred_output_filename, val_uncovered_filename, score_log_filename)\n",
        "\n",
        "            !cat {score_log_filename}\n",
        "            \n",
        "            if telegram_notifications_enabled:\n",
        "                # в строку telegram_message записывается сообщение, которое будет отправлено в телеграме. \n",
        "                # сейчас это только тег #score и текст файла, содержащего отчет о качестве.\n",
        "                # чтобы было легче жить, стоит дописать в эту строку описание эксперимента.\n",
        "                telegram_message = \"#score\\n\"+''.join(open(score_log_filename).readlines())+'\\n'+EXP_DESCRIPTION\n",
        "\n",
        "                telegram_message_encoded = urllib.parse.quote(telegram_message)\n",
        "                !curl -i -X GET \"https://api.telegram.org/bot{bot_token}/sendMessage?chat_id={chat_id}&text={telegram_message_encoded}&parse_mode=markdown\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CAN1PTjyMn96",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ML"
      ]
    },
    {
      "metadata": {
        "id": "nhdj4zszMmC8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## set ml params"
      ]
    },
    {
      "metadata": {
        "id": "qmmL4aGkHkTG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "langs=['ast']\n",
        "tracks=['1']\n",
        "data_classes = ['test', 'dev']\n",
        "\n",
        "train_steps=1000\n",
        "valid_steps=100\n",
        "save_checkpoint_steps = valid_steps\n",
        "\n",
        "train_params = [\n",
        "    f\"-train_steps {train_steps}\",\n",
        "    f\"-valid_steps {valid_steps}\",\n",
        "    f\"-save_checkpoint_steps {save_checkpoint_steps}\",\n",
        "    f\"-world_size 1\",\n",
        "    f\"-gpu_ranks 0 1\",\n",
        "    f\"-encoder_type brnn\"\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nlmG5tshvYxG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Word-level embeddings on input approach"
      ]
    },
    {
      "metadata": {
        "id": "26OEVqAOdLXp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### data description"
      ]
    },
    {
      "metadata": {
        "id": "ZgoSDdHXdLi4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**source**\n",
        "```\n",
        "wf1\n",
        "```\n",
        "\n",
        "**target**\n",
        "```\n",
        "l1 l2 ... lN +POS +Tag1=Value1 ... +TagN=ValueN +Language=langCode\n",
        "```\n",
        "\n",
        "**uncovered**\n",
        "(tab separated)\n",
        "```\n",
        "langCode\twordForm\tlemma\tPOS\tTag1=Value1|...|TagN=ValueN\n",
        "```\n",
        "\n",
        "**prediction** raw\n",
        "```\n",
        "SENT 1: ['wf1']\n",
        "...\n",
        "[-9.2825] ['c', 'o', 'n', 'v', 'i', 'd', 'u', '+NOUN', '+Gender=Masc', '+Number=Plur', '+Language=ast']\n",
        "```\n",
        "**prediction** passed to `eval()`\n",
        "```\n",
        "['l1', 'l2', ..., 'lN', '+POS', '+Tag1=Value1', ..., '+TagN=ValueN', '+Language=langCode']\n",
        "```\n",
        "\n",
        "prediction then is converted to follow the uncovered file pattern\n",
        "```\n",
        "langCode\twordForm\tlemma\tPOS\tTag1=Value1|...|TagN=ValueN\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "Sm3gwK1ZdGXU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### embeddings\n",
        "* word-level input embeddings\n",
        "* character-level output embeddings\n",
        "* learned\n",
        "* initialized with random"
      ]
    },
    {
      "metadata": {
        "id": "Owp5OkRndpCB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### data modification"
      ]
    },
    {
      "metadata": {
        "id": "H_0PpO0j-sO7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TrainDataModifyer:\n",
        "    def modify_src_line(line):\n",
        "        wf = line.split(' ')\n",
        "        return ''.join(wf)\n",
        "\n",
        "\n",
        "    def restore_src_line(line):\n",
        "        wf = line\n",
        "        return ' '.join([c for c in wf])\n",
        "\n",
        "\n",
        "    def modify_tgt_line(line):\n",
        "        return line\n",
        "    \n",
        "\n",
        "    def restore_tgt_line(line):\n",
        "        return line\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class NBestDataModifyer:\n",
        "    \n",
        "    def sent_to_baseline_compatible(line):\n",
        "        line_pref = line.split(\"[\")[0]+\"[\"\n",
        "        line_sent = line.split(\"[\")[1].rstrip(\"]\").strip('\\'')\n",
        "        return line_pref+ ', '.join('\\''+c+'\\'' for c in line_sent)+\"]\"\n",
        "                       \n",
        "    def hyp_to_baseline_compatible(line):\n",
        "        return line\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5-X-9iSHdFkT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### ml"
      ]
    },
    {
      "metadata": {
        "id": "0jh44q5HdkzS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ml(langs, tracks, train_params, TrainDataModifyer, NBestDataModifyer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HzI1DsfDuV1y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# sandbox"
      ]
    },
    {
      "metadata": {
        "id": "o7avJ-ELuVJX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}